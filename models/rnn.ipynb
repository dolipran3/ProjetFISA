{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rnn-title",
   "metadata": {},
   "source": [
    "# Simple RNN — Prédiction autorégressiive et Vanishing Gradient\n",
    "\n",
    "## Objectif\n",
    "Ce notebook construit un **SimpleRNN** pour prédire `Total_reservations`,\n",
    "tout en illustrant le **problème du vanishing gradient**.\n",
    "\n",
    "## Formule du SimpleRNN\n",
    "$$h_t = \\tanh(W_{ih}\\, x_t + b_{ih} + W_{hh}\\, h_{t-1} + b_{hh})$$\n",
    "\n",
    "## Pourquoi le vanishing gradient ?\n",
    "Lors de la BPTT (BackPropagation Through Time), le gradient doit traverser\n",
    "le produit de matrices récurrentes :\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_T}\n",
    "\\prod_{t=2}^{T} W_{hh}^\\top \\mathrm{diag}(\\tanh'(h_{t-1}))$$\n",
    "- $\\tanh'(x) \\leq 1$ → chaque facteur atténue le gradient\n",
    "- Si $\\|W_{hh}\\| < 1$ → le gradient **disparaît exponentiellement** en $T$\n",
    "- Le réseau oublie les dépendances éloignées → LSTM/GRU nécessaires\n",
    "\n",
    "## Paramètres comparables inter-modèles\n",
    "| Paramètre | Valeur | Rôle |\n",
    "|-----------|--------|------|\n",
    "| `WINDOW` | 5 | Jours de contexte (1 semaine ouvrée) |\n",
    "| `N_TEST` | 5 | Jours à prédire (dernière semaine) |\n",
    "| `TARGET_COL` | `Total_reservations` | Variable cible |\n",
    "| `EPOCHS` | 200 | Epochs max d'entraînement |\n",
    "| `HIDDEN_SIZE` | 64 | Dimension de l'état caché |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s1",
   "metadata": {},
   "source": [
    "## 1 — Configuration globale\n",
    "\n",
    "**Modifiez uniquement ce bloc** pour changer les hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-cfg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────\n",
    "# PARAMÈTRES GLOBAUX — modifier ici pour ajuster l'expérience\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "DATA_PATH   = '../data/df_venues_final.csv'\n",
    "TARGET_COL  = 'GLOBAL'\n",
    "MODEL_NAME  = 'RNN'\n",
    "\n",
    "WINDOW      = 5      # jours de contexte (fenêtre glissante)\n",
    "N_TEST      = 5      # jours de test = dernière semaine\n",
    "\n",
    "# Entraînement\n",
    "EPOCHS      = 200\n",
    "LR          = 1e-3\n",
    "BATCH_SIZE  = 16\n",
    "WEIGHT_DECAY = 1e-5\n",
    "PATIENCE    = 20\n",
    "\n",
    "# Architecture RNN\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS  = 1     # SimpleRNN : 1 couche (déjà complexe pour vanishing gradient)\n",
    "DROPOUT     = 0.0   # pas de dropout sur 1 couche\n",
    "\n",
    "# Expérience vanishing gradient : tailles de fenêtre à tester\n",
    "WINDOWS_EXP     = [2, 5, 10, 20]\n",
    "EPOCHS_EXP      = 50\n",
    "\n",
    "FEATURES = [\n",
    "    'GLOBAL',\n",
    "    'Total_reservations', 'Temp', 'pluie', 'autre',\n",
    "    'Greve_nationale', 'prof_nationale',\n",
    "    'jour_ferie.', 'pont.conge.', 'holiday',\n",
    "    'jour_lundi', 'jour_mardi', 'jour_mercredi', 'jour_jeudi', 'jour_vendredi',\n",
    "]\n",
    "TARGET_IDX = 0\n",
    "\n",
    "DAY_LABELS = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi']\n",
    "IMG_PATH   = f'../img/{MODEL_NAME.lower()}_results.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s2",
   "metadata": {},
   "source": [
    "## 2 — Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'PyTorch : {torch.__version__} | Device : {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s3",
   "metadata": {},
   "source": [
    "## 3 — Chargement et tri chronologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=';')\n",
    "\n",
    "day_order = {\n",
    "    'jour_lundi': 0, 'jour_mardi': 1, 'jour_mercredi': 2,\n",
    "    'jour_jeudi': 3, 'jour_vendredi': 4,\n",
    "}\n",
    "for col in day_order:\n",
    "    df[col] = df[col].map({'True': 1, 'False': 0}).fillna(df[col]).astype(int)\n",
    "\n",
    "df['day_num'] = df[list(day_order.keys())].idxmax(axis=1).map(day_order)\n",
    "df = df.sort_values(['Annee', 'Semaine', 'day_num']).reset_index(drop=True)\n",
    "\n",
    "print(f'Shape : {df.shape}')\n",
    "_last = df.iloc[-N_TEST:]\n",
    "TEST_LABEL = f\"S{int(_last.iloc[0]['Semaine'])}/{int(_last.iloc[0]['Annee'])}\"\n",
    "print(f'Période : {df[\"Annee\"].min()}/S{df[\"Semaine\"].min()} → {df[\"Annee\"].max()}/S{df[\"Semaine\"].max()}')\n",
    "print(f'\\n--- Dernière semaine de test ({TEST_LABEL}) ---')\n",
    "print(df[['Annee', 'Semaine', 'day_num', TARGET_COL]].tail(N_TEST).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s4",
   "metadata": {},
   "source": [
    "## 4 — Prétraitement et séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler   = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(df[FEATURES].astype(float))\n",
    "\n",
    "X_train_raw = X_scaled[:-N_TEST]\n",
    "X_test_raw  = X_scaled[-N_TEST:]\n",
    "\n",
    "def make_sequences(X, window):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i - window : i])\n",
    "        ys.append(X[i, TARGET_IDX])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "X_seq, y_seq = make_sequences(X_train_raw, WINDOW)\n",
    "\n",
    "print(f'Séquences train : {X_seq.shape}  →  {y_seq.shape}')\n",
    "\n",
    "X_tr = torch.tensor(X_seq).to(DEVICE)\n",
    "y_tr = torch.tensor(y_seq).to(DEVICE)\n",
    "train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s5",
   "metadata": {},
   "source": [
    "## 5 — Architecture SimpleRNN\n",
    "\n",
    "```\n",
    "Input  : (batch, WINDOW=5, n_features=15)\n",
    "   ↓\n",
    "RNN(hidden=64, layers=1, nonlinearity='tanh')\n",
    "   ↓  h_T  (batch, 64)\n",
    "Linear(64 → 1)\n",
    "   ↓\n",
    "Total_reservations normalisé [0, 1]\n",
    "```\n",
    "\n",
    "**Différence vs LSTM/GRU :** aucune porte de mémoire → le gradient disparaît sur de longues séquences.\n",
    "On capture les gradients de `W_hh` (poids récurrents) pour visualiser ce phénomène."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-arch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=HIDDEN_SIZE,\n",
    "                 num_layers=NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            nonlinearity='tanh',\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "\n",
    "model = SimpleRNNModel(input_size=len(FEATURES)).to(DEVICE)\n",
    "print(model)\n",
    "print(f'\\nNombre de paramètres : {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'(vs LSTM ≈ 54k, GRU ≈ 40k)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s6",
   "metadata": {},
   "source": [
    "## 6 — Entraînement + capture des gradients\n",
    "\n",
    "On enregistre la **norme L2 du gradient** de `W_hh` (poids récurrents) à chaque epoch.\n",
    "C'est cette matrice qui est répétitivement multipliée lors de la BPTT → source du vanishing gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=PATIENCE, factor=0.5\n",
    ")\n",
    "\n",
    "loss_history = []\n",
    "grad_history = {'rnn.weight_ih_l0': [], 'rnn.weight_hh_l0': [], 'fc.weight': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss /= len(X_seq)\n",
    "    loss_history.append(epoch_loss)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in grad_history and param.grad is not None:\n",
    "            grad_history[name].append(param.grad.norm().item())\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        lr_cur = optimizer.param_groups[0]['lr']\n",
    "        whh_grad = grad_history['rnn.weight_hh_l0'][-1] if grad_history['rnn.weight_hh_l0'] else 0\n",
    "        print(f'Epoch {epoch:3d}/{EPOCHS} | Loss : {epoch_loss:.5f} | '\n",
    "              f'LR : {lr_cur:.6f} | ‖∇W_hh‖ : {whh_grad:.6f}')\n",
    "\n",
    "print('\\nEntraînement terminé.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s7",
   "metadata": {},
   "source": [
    "## 7 — Prédiction autorégressiive (fenêtre glissante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-pred",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "seed_window = X_scaled[-N_TEST - WINDOW : -N_TEST].copy()\n",
    "preds_norm  = []\n",
    "window      = seed_window.copy()\n",
    "\n",
    "print('=== Prédiction autorégressiive ===')\n",
    "print(f\"{'Étape':<6} {'Jour':<10} {'ŷ norm':>8}   Fenêtre {TARGET_COL} [normalisé]\")\n",
    "print('-' * 70)\n",
    "\n",
    "for step in range(N_TEST):\n",
    "    x = torch.tensor(window, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred_n = model(x).item()\n",
    "    preds_norm.append(pred_n)\n",
    "\n",
    "    print(f'  {step}     {DAY_LABELS[step]:<10} {pred_n:>8.4f}   '\n",
    "          f'{np.round(window[:, TARGET_IDX], 3)}')\n",
    "\n",
    "    new_day             = X_test_raw[step].copy()\n",
    "    new_day[TARGET_IDX] = pred_n\n",
    "    window = np.vstack([window[1:], new_day.reshape(1, -1)])\n",
    "\n",
    "dummy                = np.zeros((N_TEST, len(FEATURES)))\n",
    "dummy[:, TARGET_IDX] = preds_norm\n",
    "preds_real = scaler.inverse_transform(dummy)[:, TARGET_IDX]\n",
    "real_real  = df[TARGET_COL].values[-N_TEST:]\n",
    "\n",
    "print('\\n=== Résultats ===')\n",
    "print(f\"{'Jour':<10} | {'Réel':>6} | {MODEL_NAME:>6} | {'Erreur':>7} | {'Erreur %':>8}\")\n",
    "print('-' * 48)\n",
    "for i, d in enumerate(DAY_LABELS):\n",
    "    err    = preds_real[i] - real_real[i]\n",
    "    err_pc = 100 * abs(err) / real_real[i]\n",
    "    print(f'{d:<10} | {real_real[i]:>6.0f} | {preds_real[i]:>6.0f} | {err:>+7.0f} | {err_pc:>7.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s8",
   "metadata": {},
   "source": [
    "## 8 — Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae  = mean_absolute_error(real_real, preds_real)\n",
    "mape = mean_absolute_percentage_error(real_real, preds_real) * 100\n",
    "rmse = np.sqrt(np.mean((real_real - preds_real) ** 2))\n",
    "r2   = r2_score(real_real, preds_real)\n",
    "\n",
    "print(f'╔{\"═\" * 50}╗')\n",
    "print(f'║  Métriques {MODEL_NAME} — {TEST_LABEL:<{37}}║')\n",
    "print(f'╠{\"═\" * 50}╣')\n",
    "print(f'║  MAE   (Erreur absolue moyenne)  : {mae:>7.1f} réserv. ║')\n",
    "print(f'║  MAPE  (Erreur % moyenne)        : {mape:>6.2f}%          ║')\n",
    "print(f'║  RMSE  (Racine erreur quadrat.)  : {rmse:>7.1f} réserv. ║')\n",
    "print(f'║  R²    (Coefficient det.)        : {r2:>7.4f}           ║')\n",
    "print(f'╚{\"═\" * 50}╝')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s9",
   "metadata": {},
   "source": [
    "## 9 — Visualisations standardisées\n",
    "\n",
    "Identiques aux autres modèles + un graphique supplémentaire sur les gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_real     = df[TARGET_COL].values\n",
    "test_indices = list(range(len(all_real) - N_TEST, len(all_real)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f'{MODEL_NAME} — Prédiction autorégressiive ({TEST_LABEL})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# ── 1 : Courbe d'apprentissage ────────────────────────────────────────────\n",
    "ax = axes[0]\n",
    "ax.plot(loss_history, color='mediumpurple', lw=1.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE (normalisé)')\n",
    "ax.set_title('Courbe d\\'apprentissage')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.annotate(f'Loss finale : {loss_history[-1]:.5f}',\n",
    "            xy=(len(loss_history) - 1, loss_history[-1]),\n",
    "            xytext=(-80, 20), textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'), fontsize=9)\n",
    "\n",
    "# ── 2 : Série complète + prédiction ─────────────────────────────────────\n",
    "ax = axes[1]\n",
    "ax.plot(range(len(all_real)), all_real,\n",
    "        label='Réservations réelles', color='steelblue', alpha=0.7, lw=1.2)\n",
    "ax.plot(test_indices, preds_real,\n",
    "        label=f'{MODEL_NAME} autorégressif', color='mediumpurple',\n",
    "        ls='--', marker='o', ms=7, lw=2)\n",
    "ax.plot(test_indices, real_real,\n",
    "        color='steelblue', marker='x', ms=9, ls='None', label=f'Réel ({TEST_LABEL})')\n",
    "ax.axvline(x=test_indices[0] - 0.5, color='gray', ls=':', lw=1.5, label='Début test')\n",
    "ax.set_xlabel('Jours (ordre chronologique)')\n",
    "ax.set_ylabel(TARGET_COL)\n",
    "ax.set_title(f'Prédiction — fenêtre={WINDOW} jours')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "axins = ax.inset_axes([0.60, 0.05, 0.38, 0.45])\n",
    "axins.plot(DAY_LABELS, real_real, 'o-', color='steelblue', ms=5)\n",
    "axins.plot(DAY_LABELS, preds_real, 's--', color='mediumpurple', ms=5)\n",
    "axins.set_title(f'Zoom {TEST_LABEL}', fontsize=8)\n",
    "axins.tick_params(axis='x', labelsize=6)\n",
    "axins.grid(alpha=0.3)\n",
    "\n",
    "# ── 3 : Barres côte à côte ───────────────────────────────────────────────\n",
    "ax    = axes[2]\n",
    "x     = np.arange(N_TEST)\n",
    "width = 0.35\n",
    "b_r   = ax.bar(x - width/2, real_real,  width, label='Réel',      color='steelblue',   alpha=0.85)\n",
    "b_p   = ax.bar(x + width/2, preds_real, width, label=MODEL_NAME,  color='mediumpurple', alpha=0.85)\n",
    "for bar, val in zip(b_r, real_real):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f'{val:.0f}', ha='center', va='bottom', fontsize=8, color='steelblue')\n",
    "for bar, val in zip(b_p, preds_real):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f'{val:.0f}', ha='center', va='bottom', fontsize=8, color='mediumpurple')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(DAY_LABELS)\n",
    "ax.set_ylabel(TARGET_COL)\n",
    "ax.set_title('Comparaison jour par jour')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i in range(N_TEST):\n",
    "    err = preds_real[i] - real_real[i]\n",
    "    ax.text(i, max(real_real[i], preds_real[i]) + 12,\n",
    "            f'{err:+.0f}', ha='center', fontsize=7, color='darkred')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_PATH, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure sauvegardée → {IMG_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s10",
   "metadata": {},
   "source": [
    "## 10 — Analyse du Vanishing Gradient\n",
    "\n",
    "### Visualisation des normes de gradients par couche\n",
    "On compare :\n",
    "- `W_ih` : poids entrée → caché (gradient de surface)\n",
    "- `W_hh` : **poids récurrents** (source du vanishing)\n",
    "- `W_fc` : poids de la couche de sortie (gradient fort, proche de la loss)\n",
    "\n",
    "On s'attend à voir `W_hh` avec une norme bien plus faible que `W_fc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-gradplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Analyse du Vanishing Gradient — SimpleRNN', fontsize=13, fontweight='bold')\n",
    "\n",
    "# ── Normes de gradients par couche ────────────────────────────────────────\n",
    "ax = axes[0]\n",
    "colors = {'rnn.weight_ih_l0': 'steelblue', 'rnn.weight_hh_l0': 'tomato', 'fc.weight': 'green'}\n",
    "labels = {'rnn.weight_ih_l0': 'W_ih  (entrée → caché)',\n",
    "          'rnn.weight_hh_l0': 'W_hh  (récurrent — vanishing !)',\n",
    "          'fc.weight'        : 'W_fc  (couche de sortie)'}\n",
    "for name, grads in grad_history.items():\n",
    "    if grads:\n",
    "        ax.plot(grads, label=labels[name], color=colors[name], lw=1.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('‖∇W‖₂')\n",
    "ax.set_title('Norme des gradients par couche')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# ── Vanishing gradient selon la longueur de fenêtre ───────────────────────\n",
    "ax = axes[1]\n",
    "\n",
    "results_vg = {}\n",
    "for win in WINDOWS_EXP:\n",
    "    Xs, ys = make_sequences(X_scaled[:-N_TEST], win)\n",
    "    Xt = torch.tensor(Xs, dtype=torch.float32).to(DEVICE)\n",
    "    yt = torch.tensor(ys, dtype=torch.float32).to(DEVICE)\n",
    "    loader_w = DataLoader(TensorDataset(Xt, yt), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    m = SimpleRNNModel(len(FEATURES), HIDDEN_SIZE, 1).to(DEVICE)\n",
    "    opt_w = torch.optim.Adam(m.parameters(), lr=LR)\n",
    "    crit  = nn.MSELoss()\n",
    "    grad_norms_hh = []\n",
    "\n",
    "    for ep in range(EPOCHS_EXP):\n",
    "        m.train()\n",
    "        for xb, yb in loader_w:\n",
    "            opt_w.zero_grad()\n",
    "            loss = crit(m(xb), yb)\n",
    "            loss.backward()\n",
    "            opt_w.step()\n",
    "        if m.rnn.weight_hh_l0.grad is not None:\n",
    "            grad_norms_hh.append(m.rnn.weight_hh_l0.grad.norm().item())\n",
    "    results_vg[win] = grad_norms_hh\n",
    "\n",
    "palette = plt.cm.viridis(np.linspace(0.2, 0.9, len(WINDOWS_EXP)))\n",
    "for color, win in zip(palette, WINDOWS_EXP):\n",
    "    if results_vg[win]:\n",
    "        ax.plot(results_vg[win], label=f'T = {win}', color=color, lw=1.5)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('‖∇W_hh‖₂  (log scale)')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Vanishing Gradient vs longueur de fenêtre')\n",
    "ax.legend(title='Taille fenêtre', fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../img/rnn_vanishing_gradient.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Sauvegardé dans img/rnn_vanishing_gradient.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-s11",
   "metadata": {},
   "source": [
    "## 11 — Conclusion et comparaison avec LSTM/GRU\n",
    "\n",
    "| Modèle | Paramètres | Mémoire | Vanishing gradient |\n",
    "|--------|-----------|---------|--------------------|\n",
    "| **SimpleRNN** | ~1.5k | Courte (~3-5 pas) | **Oui** (critique) |\n",
    "| **GRU** | ~40k | Longue | Non (porte de mise à jour) |\n",
    "| **LSTM** | ~54k | Longue | Non (cellule de contexte) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=== Trace de la fenêtre glissante — {MODEL_NAME} ===')\n",
    "print(f\"{'Étape':<7} {'Jour':<12} {'Réel':>6} {'Prédit':>7} {'Err abs':>8} {'Err %':>7}   Rôle\")\n",
    "print('-' * 75)\n",
    "for i, d in enumerate(DAY_LABELS):\n",
    "    err_abs = abs(preds_real[i] - real_real[i])\n",
    "    err_pct = 100 * err_abs / real_real[i]\n",
    "    role = '← injecté dans la fenêtre suivante' if i < N_TEST - 1 else '← prédiction finale'\n",
    "    print(f'  {i}      {d:<12} {real_real[i]:>6.0f} {preds_real[i]:>7.0f} {err_abs:>8.1f} {err_pct:>6.1f}%   {role}')\n",
    "\n",
    "print(f'\\nRésumé : MAE={mae:.1f} | MAPE={mape:.2f}% | RMSE={rmse:.1f} | R²={r2:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
