{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xgb-title",
   "metadata": {},
   "source": [
    "# XGBoost — Baseline Prédiction autorégressiive (fenêtres glissantes)\n",
    "\n",
    "## Objectif\n",
    "Ce notebook implémente un **XGBoost** comme modèle de référence (baseline) pour la prédiction\n",
    "autorégressiive de `Total_reservations`.\n",
    "\n",
    "## Approche — fenêtres glissantes aplaties\n",
    "XGBoost est un modèle **non-séquentiel** : on transforme la série temporelle en\n",
    "données tabulaires en **aplatissant** la fenêtre glissante :\n",
    "\n",
    "```\n",
    "Fenêtre (WINDOW × n_features)  →  aplatissement  →  vecteur de WINDOW*n_features valeurs\n",
    "XGBRegressor → prédiction scalaire Total_reservations\n",
    "```\n",
    "\n",
    "**Pas de normalisation nécessaire** : XGBoost est invariant aux transformations monotones.\n",
    "\n",
    "## Stratégie autorégressiive\n",
    "Identique aux modèles DL : le `Total_reservations` prédit remplace la valeur réelle\n",
    "dans la fenêtre à chaque étape. Les features exogènes sont connues à l'avance.\n",
    "\n",
    "## Paramètres comparables inter-modèles\n",
    "| Paramètre | Valeur | Rôle |\n",
    "|-----------|--------|------|\n",
    "| `WINDOW` | 5 | Jours de contexte (1 semaine ouvrée) |\n",
    "| `N_TEST` | 5 | Jours à prédire (dernière semaine) |\n",
    "| `TARGET_COL` | `Total_reservations` | Variable cible |\n",
    "\n",
    "## Avantages du XGBoost comme baseline\n",
    "- Pas d'entraînement itératif → rapide et reproductible\n",
    "- Interprétable via l'**importance des features**\n",
    "- Robuste aux outliers et aux valeurs manquantes\n",
    "- Donne une borne inférieure de performance attendue des modèles DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s1",
   "metadata": {},
   "source": [
    "## 1 — Configuration globale\n",
    "\n",
    "**Modifiez uniquement ce bloc** pour changer les hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-cfg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────\n",
    "# PARAMÈTRES GLOBAUX — modifier ici pour ajuster l'expérience\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "DATA_PATH   = '../data/df_venues_final.csv'\n",
    "TARGET_COL  = 'GLOBAL'\n",
    "MODEL_NAME  = 'XGBoost'\n",
    "\n",
    "WINDOW      = 5      # jours de contexte (fenêtre glissante aplatie)\n",
    "N_TEST      = 5      # jours de test = dernière semaine\n",
    "\n",
    "# Hyperparamètres XGBoost\n",
    "N_ESTIMATORS     = 300\n",
    "MAX_DEPTH        = 4\n",
    "LEARNING_RATE    = 0.05\n",
    "SUBSAMPLE        = 0.8\n",
    "COLSAMPLE_BYTREE = 0.8\n",
    "MIN_CHILD_WEIGHT = 2\n",
    "REG_ALPHA        = 0.1    # régularisation L1\n",
    "REG_LAMBDA       = 1.0    # régularisation L2\n",
    "RANDOM_STATE     = 42\n",
    "N_JOBS           = -1     # utiliser tous les cores disponibles\n",
    "\n",
    "FEATURES = [\n",
    "    'GLOBAL',               # idx 0 — feedback autorégressif\n",
    "    'Total_reservations', 'Temp', 'pluie', 'autre',\n",
    "    'Greve_nationale', 'prof_nationale',\n",
    "    'jour_ferie.', 'pont.conge.', 'holiday',\n",
    "    'jour_lundi', 'jour_mardi', 'jour_mercredi', 'jour_jeudi', 'jour_vendredi',\n",
    "]\n",
    "TARGET_IDX = 0\n",
    "\n",
    "DAY_LABELS = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi']\n",
    "IMG_PATH   = f'../img/{MODEL_NAME.lower()}_results.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s2",
   "metadata": {},
   "source": [
    "## 2 — Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('XGBoost et dépendances importés ✓')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s3",
   "metadata": {},
   "source": [
    "## 3 — Chargement et tri chronologique\n",
    "\n",
    "Même pipeline de préparation que les modèles DL pour une comparaison équitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=';')\n",
    "\n",
    "day_order = {\n",
    "    'jour_lundi': 0, 'jour_mardi': 1, 'jour_mercredi': 2,\n",
    "    'jour_jeudi': 3, 'jour_vendredi': 4,\n",
    "}\n",
    "for col in day_order:\n",
    "    df[col] = df[col].map({'True': 1, 'False': 0}).fillna(df[col]).astype(int)\n",
    "\n",
    "df['day_num'] = df[list(day_order.keys())].idxmax(axis=1).map(day_order)\n",
    "df = df.sort_values(['Annee', 'Semaine', 'day_num']).reset_index(drop=True)\n",
    "\n",
    "print(f'Shape : {df.shape}')\n",
    "_last = df.iloc[-N_TEST:]\n",
    "TEST_LABEL = f\"S{int(_last.iloc[0]['Semaine'])}/{int(_last.iloc[0]['Annee'])}\"\n",
    "print(f'Période : {df[\"Annee\"].min()}/S{df[\"Semaine\"].min()} → {df[\"Annee\"].max()}/S{df[\"Semaine\"].max()}')\n",
    "print(f'\\n--- Dernière semaine de test ({TEST_LABEL}) ---')\n",
    "print(df[['Annee', 'Semaine', 'day_num', TARGET_COL]].tail(N_TEST).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s4",
   "metadata": {},
   "source": [
    "## 4 — Construction des fenêtres glissantes (format tabulaire)\n",
    "\n",
    "**Différence clé vs DL :** au lieu de conserver la structure `(N, WINDOW, n_features)`,\n",
    "on **aplatit** chaque fenêtre en un vecteur `(N, WINDOW × n_features)`.\n",
    "\n",
    "```\n",
    "Fenêtre [jour_0, jour_1, ..., jour_{W-1}]  (W × F dimensions)\n",
    "         ↓  aplatissement (flatten)\n",
    "Vecteur  [f00, f01, ..., f0F, f10, f11, ..., f_{W-1,F}]  (W×F dimensions)\n",
    "```\n",
    "\n",
    "Les noms de features incluent le décalage temporel (`_d0`, `_d1`, ...) pour interpréter\n",
    "l'importance XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[FEATURES].values.astype(float)  # pas de normalisation pour XGBoost\n",
    "\n",
    "n_train = len(df) - N_TEST\n",
    "train_data = data[:n_train]\n",
    "test_data  = data[n_train:]\n",
    "\n",
    "def make_flat_sequences(X, window):\n",
    "    \"\"\"Construit des fenêtres glissantes aplaties pour XGBoost (format tabulaire).\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(window, len(X)):\n",
    "        Xs.append(X[i - window : i].flatten())  # aplatissement\n",
    "        ys.append(X[i, TARGET_IDX])\n",
    "    return np.array(Xs, dtype=float), np.array(ys, dtype=float)\n",
    "\n",
    "X_train, y_train = make_flat_sequences(train_data, WINDOW)\n",
    "\n",
    "# Noms de features pour l'importance\n",
    "feature_names = [f'{col}_d{k}' for k in range(WINDOW) for col in FEATURES]\n",
    "\n",
    "print(f'Échantillons train : {X_train.shape[0]}  |  Features aplaties : {X_train.shape[1]}')\n",
    "print(f'Dernière semaine test : {TEST_LABEL}')\n",
    "print(f'\\nFenêtre initiale de prédiction (jours {n_train - WINDOW}–{n_train - 1}) :')\n",
    "print(df[['Annee', 'Semaine', 'day_num', TARGET_COL]].iloc[n_train - WINDOW:n_train].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s5",
   "metadata": {},
   "source": [
    "## 5 — Entraînement XGBoost\n",
    "\n",
    "Le `XGBRegressor` est entraîné en une seule passe (pas d'entraînement itératif par epoch).\n",
    "La métrique d'entraînement est calculée sur l'ensemble du train pour évaluer l'ajustement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    subsample=SUBSAMPLE,\n",
    "    colsample_bytree=COLSAMPLE_BYTREE,\n",
    "    min_child_weight=MIN_CHILD_WEIGHT,\n",
    "    reg_alpha=REG_ALPHA,\n",
    "    reg_lambda=REG_LAMBDA,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbosity=0,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Métriques sur le train (vérification d'ajustement)\n",
    "train_pred = model.predict(X_train)\n",
    "train_mae  = mean_absolute_error(y_train, train_pred)\n",
    "train_mape = mean_absolute_percentage_error(y_train, train_pred) * 100\n",
    "train_rmse = np.sqrt(np.mean((y_train - train_pred) ** 2))\n",
    "train_r2   = r2_score(y_train, train_pred)\n",
    "\n",
    "print(f'Train — MAE : {train_mae:.1f}  |  MAPE : {train_mape:.1f}%  |  RMSE : {train_rmse:.1f}  |  R² : {train_r2:.4f}')\n",
    "print(f'\\nNombre d\\'estimateurs : {N_ESTIMATORS}  |  Max depth : {MAX_DEPTH}  |  LR : {LEARNING_RATE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s6",
   "metadata": {},
   "source": [
    "## 6 — Prédiction autorégressiive (fenêtre glissante)\n",
    "\n",
    "**Pour chaque jour `t` de la semaine cible :**\n",
    "\n",
    "1. Aplatir la fenêtre courante `(WINDOW × n_features)` → vecteur\n",
    "2. Passer dans XGBoost → `ŷ_t` (valeur brute, pas normalisée)\n",
    "3. Créer le vecteur du jour `t` : features exogènes réelles + `Total_reservations = ŷ_t`\n",
    "4. Glisser la fenêtre\n",
    "\n",
    "**Note :** contrairement aux modèles DL, pas besoin de dénormalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-pred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fenêtre initiale : WINDOW jours réels avant la semaine de test\n",
    "window_rows = [data[n_train - WINDOW + k].copy() for k in range(WINDOW)]\n",
    "\n",
    "preds_real = []\n",
    "real_real  = df[TARGET_COL].values[-N_TEST:]\n",
    "\n",
    "print('=== Prédiction autorégressiive (fenêtre glissante) ===')\n",
    "print(f\"{'Étape':<6} {'Jour':<10} {'Prédit':>8} {'Réel':>8} {'Err abs':>9} {'Err %':>8}\")\n",
    "print('-' * 56)\n",
    "\n",
    "for step in range(N_TEST):\n",
    "    # Aplatissement de la fenêtre\n",
    "    x_input = np.array(window_rows, dtype=float).flatten().reshape(1, -1)\n",
    "    pred    = float(model.predict(x_input)[0])\n",
    "    actual  = float(real_real[step])\n",
    "    preds_real.append(pred)\n",
    "\n",
    "    err_abs = abs(actual - pred)\n",
    "    err_pct = 100 * err_abs / actual\n",
    "    print(f'{step+1:<6} {DAY_LABELS[step]:<10} {pred:>8.0f} {actual:>8.0f} {err_abs:>9.1f} {err_pct:>7.1f}%')\n",
    "\n",
    "    # Nouveau jour : features exogènes réelles + Total_reservations prédit\n",
    "    new_row             = test_data[step].copy()\n",
    "    new_row[TARGET_IDX] = pred\n",
    "    window_rows.pop(0)\n",
    "    window_rows.append(new_row)\n",
    "\n",
    "preds_real = np.array(preds_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s7",
   "metadata": {},
   "source": [
    "## 7 — Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae  = mean_absolute_error(real_real, preds_real)\n",
    "mape = mean_absolute_percentage_error(real_real, preds_real) * 100\n",
    "rmse = np.sqrt(np.mean((real_real - preds_real) ** 2))\n",
    "r2   = r2_score(real_real, preds_real)\n",
    "\n",
    "print(f'╔{\"═\" * 50}╗')\n",
    "print(f'║  Métriques {MODEL_NAME} — {TEST_LABEL:<{37}}║')\n",
    "print(f'╠{\"═\" * 50}╣')\n",
    "print(f'║  MAE   (Erreur absolue moyenne)  : {mae:>7.1f} réserv. ║')\n",
    "print(f'║  MAPE  (Erreur % moyenne)        : {mape:>6.2f}%          ║')\n",
    "print(f'║  RMSE  (Racine erreur quadrat.)  : {rmse:>7.1f} réserv. ║')\n",
    "print(f'║  R²    (Coefficient det.)        : {r2:>7.4f}           ║')\n",
    "print(f'╚{\"═\" * 50}╝')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s8",
   "metadata": {},
   "source": [
    "## 8 — Visualisations\n",
    "\n",
    "Trois graphiques standardisés :\n",
    "1. **Importance des features** (top 15) — remplace la courbe d'apprentissage car XGBoost\n",
    "   est entraîné en une passe (pas d'itérations par epoch).\n",
    "   L'importance est mesurée par le **gain moyen** : contribution de chaque feature\n",
    "   à la réduction de la loss lors des splits.\n",
    "2. **Série complète + prédiction**\n",
    "3. **Barres côte à côte jour par jour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_real     = df[TARGET_COL].values\n",
    "test_indices = list(range(len(all_real) - N_TEST, len(all_real)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f'{MODEL_NAME} — Prédiction autorégressiive ({TEST_LABEL})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# ── 1 : Importance des features (top 15) ─────────────────────────────────\n",
    "ax = axes[0]\n",
    "importances = model.feature_importances_\n",
    "top_n = 15\n",
    "sorted_idx = np.argsort(importances)[::-1][:top_n]\n",
    "top_names  = [feature_names[i] for i in sorted_idx]\n",
    "top_vals   = importances[sorted_idx]\n",
    "\n",
    "ax.barh(top_names[::-1], top_vals[::-1], color='forestgreen', alpha=0.8)\n",
    "ax.set_xlabel('Importance (gain)')\n",
    "ax.set_title(f'Top {top_n} features — {MODEL_NAME}')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# ── 2 : Série complète + prédiction ─────────────────────────────────────\n",
    "ax = axes[1]\n",
    "ax.plot(range(len(all_real)), all_real,\n",
    "        label='Réservations réelles', color='steelblue', alpha=0.7, lw=1.2)\n",
    "ax.plot(test_indices, preds_real,\n",
    "        label=f'{MODEL_NAME} autorégressif', color='forestgreen',\n",
    "        ls='--', marker='o', ms=7, lw=2)\n",
    "ax.plot(test_indices, real_real,\n",
    "        color='steelblue', marker='x', ms=9, ls='None', label=f'Réel ({TEST_LABEL})')\n",
    "ax.axvline(x=test_indices[0] - 0.5, color='gray', ls=':', lw=1.5, label='Début test')\n",
    "ax.set_xlabel('Jours (ordre chronologique)')\n",
    "ax.set_ylabel(TARGET_COL)\n",
    "ax.set_title(f'Prédiction — fenêtre={WINDOW} jours')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "axins = ax.inset_axes([0.60, 0.05, 0.38, 0.45])\n",
    "axins.plot(DAY_LABELS, real_real, 'o-', color='steelblue', ms=5)\n",
    "axins.plot(DAY_LABELS, preds_real, 's--', color='forestgreen', ms=5)\n",
    "axins.set_title(f'Zoom {TEST_LABEL}', fontsize=8)\n",
    "axins.tick_params(axis='x', labelsize=6)\n",
    "axins.grid(alpha=0.3)\n",
    "\n",
    "# ── 3 : Barres côte à côte ───────────────────────────────────────────────\n",
    "ax    = axes[2]\n",
    "x     = np.arange(N_TEST)\n",
    "width = 0.35\n",
    "b_r   = ax.bar(x - width/2, real_real,  width, label='Réel',       color='steelblue',  alpha=0.85)\n",
    "b_p   = ax.bar(x + width/2, preds_real, width, label=MODEL_NAME,   color='forestgreen', alpha=0.85)\n",
    "for bar, val in zip(b_r, real_real):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f'{val:.0f}', ha='center', va='bottom', fontsize=8, color='steelblue')\n",
    "for bar, val in zip(b_p, preds_real):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f'{val:.0f}', ha='center', va='bottom', fontsize=8, color='forestgreen')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(DAY_LABELS)\n",
    "ax.set_ylabel(TARGET_COL)\n",
    "ax.set_title('Comparaison jour par jour')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i in range(N_TEST):\n",
    "    err = preds_real[i] - real_real[i]\n",
    "    ax.text(i, max(real_real[i], preds_real[i]) + 12,\n",
    "            f'{err:+.0f}', ha='center', fontsize=7, color='darkred')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_PATH, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure sauvegardée → {IMG_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb-s9",
   "metadata": {},
   "source": [
    "## 9 — Trace de la fenêtre glissante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=== Trace de la fenêtre glissante — {MODEL_NAME} ===')\n",
    "print(f\"{'Étape':<7} {'Jour':<12} {'Réel':>6} {'Prédit':>7} {'Err abs':>8} {'Err %':>7}   Rôle\")\n",
    "print('-' * 75)\n",
    "for i, d in enumerate(DAY_LABELS):\n",
    "    err_abs = abs(preds_real[i] - real_real[i])\n",
    "    err_pct = 100 * err_abs / real_real[i]\n",
    "    role = '← injecté dans la fenêtre suivante' if i < N_TEST - 1 else '← prédiction finale'\n",
    "    print(f'  {i}      {d:<12} {real_real[i]:>6.0f} {preds_real[i]:>7.0f} {err_abs:>8.1f} {err_pct:>6.1f}%   {role}')\n",
    "\n",
    "print(f'\\nRésumé : MAE={mae:.1f} | MAPE={mape:.2f}% | RMSE={rmse:.1f} | R²={r2:.4f}')"
   ]
  }
 ]
}