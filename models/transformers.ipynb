{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tr-title",
   "metadata": {},
   "source": [
    "# Modèles Transformer — Comparaison PatchTST, TimeXer, iTransformer, VanillaTransformer\n",
    "\n",
    "## Objectif\n",
    "Comparer 4 architectures Transformer modernes pour la prédiction autorégressiive de `Total_reservations`.\n",
    "\n",
    "## Modèles implémentés\n",
    "\n",
    "| Modèle | Papier | Idée clé |\n",
    "|--------|--------|----------|\n",
    "| **VanillaTransformer** | Vaswani 2017 | Token = pas de temps multivarié |\n",
    "| **PatchTST** | Nie et al. ICLR 2023 | Token = *patch* (sous-séquence) de la série |\n",
    "| **TimeXer** | Chen et al. NeurIPS 2024 | PatchTST + Cross-Attention sur les variables exogènes |\n",
    "| **iTransformer** | Liu et al. ICLR 2024 | Token = *variable* entière (Transformer inversé) |\n",
    "\n",
    "## Stratégie endogène / exogène\n",
    "- **Série endogène** : `Total_reservations` — la variable à prédire\n",
    "- **Features exogènes** : toutes les autres colonnes (météo, jours fériés, jours de semaine…)\n",
    "  — supposées connues à l'avance (calendaire + prévision météo)\n",
    "\n",
    "## Paramètres comparables inter-modèles\n",
    "| Paramètre | Valeur | Rôle |\n",
    "|-----------|--------|------|\n",
    "| `WINDOW` | 5 | Jours de contexte (1 semaine ouvrée) |\n",
    "| `N_TEST` | 5 | Jours à prédire (dernière semaine) |\n",
    "| `TARGET_COL` | `Total_reservations` | Variable cible |\n",
    "| `D_MODEL` | 64 | Dimension interne des Transformers |\n",
    "| `N_HEADS` | 4 | Têtes d'attention |\n",
    "| `N_LAYERS` | 2 | Couches d'encodeur |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s1",
   "metadata": {},
   "source": [
    "## 1 — Configuration globale\n",
    "\n",
    "**Modifiez uniquement ce bloc** pour changer les hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-cfg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────\n",
    "# PARAMÈTRES GLOBAUX — modifier ici pour ajuster l'expérience\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "DATA_PATH  = '../data/df_venues_final.csv'\n",
    "TARGET_COL = 'GLOBAL'               # série endogène\n",
    "\n",
    "WINDOW      = 5      # jours de contexte\n",
    "N_TEST      = 5      # jours de test\n",
    "PATCH_SIZE  = 1      # taille d'un patch (WINDOW doit être divisible par PATCH_SIZE)\n",
    "\n",
    "# Entraînement\n",
    "EPOCHS      = 200\n",
    "LR          = 3e-4\n",
    "BATCH_SIZE  = 16\n",
    "PATIENCE    = 40     # early stopping\n",
    "\n",
    "# Architecture Transformer\n",
    "D_MODEL     = 64\n",
    "N_HEADS     = 4\n",
    "N_LAYERS    = 2\n",
    "DROPOUT     = 0.1\n",
    "\n",
    "# Couleurs par modèle (cohérentes dans tous les graphiques)\n",
    "MODEL_COLORS = {\n",
    "    'VanillaTransformer': '#C44E52',\n",
    "    'PatchTST'          : '#4C72B0',\n",
    "    'TimeXer'           : '#DD8452',\n",
    "    'iTransformer'      : '#55A868',\n",
    "}\n",
    "\n",
    "DAY_LABELS = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi']\n",
    "IMG_PATH   = '../img/transformers_results.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s2",
   "metadata": {},
   "source": [
    "## 2 — Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device('mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "                       else 'cuda' if torch.cuda.is_available()\n",
    "                       else 'cpu')\n",
    "print(f'PyTorch : {torch.__version__} | Device : {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s3",
   "metadata": {},
   "source": [
    "## 3 — Chargement, préparation et normalisation\n",
    "\n",
    "Les données sont séparées en :\n",
    "- **Série endogène** : `Total_reservations` (la cible)\n",
    "- **Features exogènes** : toutes les autres colonnes\n",
    "\n",
    "On utilise `StandardScaler` (µ=0, σ=1) ajusté **uniquement sur le train** pour éviter\n",
    "la fuite d'information du test vers le train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=';')\n",
    "\n",
    "day_order = {\n",
    "    'jour_lundi': 0, 'jour_mardi': 1, 'jour_mercredi': 2,\n",
    "    'jour_jeudi': 3, 'jour_vendredi': 4,\n",
    "}\n",
    "for col in day_order:\n",
    "    df[col] = df[col].map({'True': 1, 'False': 0}).fillna(df[col]).astype(int)\n",
    "\n",
    "df['day_num'] = df[list(day_order.keys())].idxmax(axis=1).map(day_order)\n",
    "df = df.sort_values(['Annee', 'Semaine', 'day_num']).reset_index(drop=True)\n",
    "\n",
    "# Colonnes exogènes = tout sauf TARGET_COL et day_num\n",
    "EXO_COLS = [c for c in df.columns if c not in [TARGET_COL, 'day_num']]\n",
    "\n",
    "n          = len(df)\n",
    "test_start = n - N_TEST\n",
    "\n",
    "_last = df.iloc[-N_TEST:]\n",
    "TEST_LABEL = f\"S{int(_last.iloc[0]['Semaine'])}/{int(_last.iloc[0]['Annee'])}\"\n",
    "\n",
    "# Normalisation : ajustée sur le train uniquement\n",
    "scaler_endo = StandardScaler()\n",
    "scaler_exo  = StandardScaler()\n",
    "\n",
    "train_endo = scaler_endo.fit_transform(df[[TARGET_COL]].values[:test_start]).flatten()\n",
    "train_exo  = scaler_exo.fit_transform(df[EXO_COLS].values.astype(float)[:test_start])\n",
    "\n",
    "test_endo  = scaler_endo.transform(df[[TARGET_COL]].values[test_start:]).flatten()\n",
    "test_exo   = scaler_exo.transform(df[EXO_COLS].values.astype(float)[test_start:])\n",
    "\n",
    "all_endo = np.concatenate([train_endo, test_endo])\n",
    "all_exo  = np.concatenate([train_exo,  test_exo], axis=0)\n",
    "\n",
    "N_EXO = all_exo.shape[1]\n",
    "\n",
    "print(f'Shape : {df.shape}  |  Train : {test_start} jours  |  Test : {N_TEST} jours')\n",
    "print(f'Features exogènes : {N_EXO} → {EXO_COLS[:5]}...')\n",
    "print(f'Test : {TEST_LABEL}')\n",
    "print(f'\\n--- Dernière semaine de test ---')\n",
    "print(df[['Annee', 'Semaine', 'day_num', TARGET_COL]].tail(N_TEST).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s4",
   "metadata": {},
   "source": [
    "## 4 — Dataset et DataLoader\n",
    "\n",
    "La classe `TSDataset` crée des fenêtres glissantes avec séparation endogène/exogène :\n",
    "- `x_endo` : série `Total_reservations` des `WINDOW` jours précédents → `(WINDOW,)`\n",
    "- `x_exo`  : features exogènes sur la même fenêtre → `(WINDOW, N_EXO)`\n",
    "- `y`      : valeur cible (scalaire normalisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-ds",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataset(Dataset):\n",
    "    def __init__(self, endo, exo, win, limit=None):\n",
    "        self.endo    = torch.tensor(endo, dtype=torch.float32)\n",
    "        self.exo     = torch.tensor(exo,  dtype=torch.float32)\n",
    "        self.win     = win\n",
    "        max_i = limit if limit is not None else len(endo)\n",
    "        self.indices = list(range(win, max_i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        return (self.endo[i - self.win : i],    # (W,)\n",
    "                self.exo [i - self.win : i],    # (W, N_EXO)\n",
    "                self.endo[i])                   # scalaire\n",
    "\n",
    "\n",
    "train_ds = TSDataset(all_endo, all_exo, win=WINDOW, limit=test_start)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "print(f'Échantillons train : {len(train_ds)}')\n",
    "print(f'x_endo : ({BATCH_SIZE}, {WINDOW})  |  x_exo : ({BATCH_SIZE}, {WINDOW}, {N_EXO})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s5",
   "metadata": {},
   "source": [
    "## 5 — Briques communes\n",
    "\n",
    "### PositionalEncoding\n",
    "Encodage positionnel sinusoïdal fixe (Vaswani 2017) pour informer les tokens de leur position.\n",
    "\n",
    "### PatchEmbedding\n",
    "Découpe la série en patches de longueur `PATCH_SIZE` et projette chaque patch dans `D_MODEL` dimensions.\n",
    "Avec `PATCH_SIZE=1` : chaque jour = 1 token (équivalent à une projection par pas de temps).\n",
    "\n",
    "### ExoEmbedding\n",
    "Projette les features exogènes `(W, N_EXO)` → `(W, D_MODEL)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-bricks",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe  = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Découpe (B, W) en (B, n_patches, d_model) — patch_size=1 → 1 token/jour.\"\"\"\n",
    "    def __init__(self, window, patch_size, d_model):\n",
    "        super().__init__()\n",
    "        assert window % patch_size == 0, 'window doit être divisible par patch_size'\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches  = window // patch_size\n",
    "        self.proj       = nn.Linear(patch_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, W = x.shape\n",
    "        x = x.view(B, self.n_patches, self.patch_size)\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class ExoEmbedding(nn.Module):\n",
    "    \"\"\"Projette (B, W, n_exo) → (B, W, d_model).\"\"\"\n",
    "    def __init__(self, n_exo, d_model):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(n_exo, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "print('Briques communes définies ✓')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s6",
   "metadata": {},
   "source": [
    "## 6 — Architectures des 4 modèles\n",
    "\n",
    "### VanillaTransformer\n",
    "Token = pas de temps multivarié (endogène + exogènes concaténés). Self-Attention temporelle classique.\n",
    "\n",
    "### PatchTST *(Nie et al. ICLR 2023)*\n",
    "Token = patch de la série endogène. N'utilise **pas** les exogènes (univarié). La patchification\n",
    "réduit la séquence et capture la dynamique locale de chaque patch.\n",
    "\n",
    "### TimeXer *(Chen et al. NeurIPS 2024)*\n",
    "PatchTST + **Cross-Attention** : les tokens endogènes (Q) interrogent les tokens exogènes (K, V).\n",
    "$$\\text{CrossAttn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### iTransformer *(Liu et al. ICLR 2024)*\n",
    "Token = **variable entière** (toute la série temporelle d'une variable). Self-Attention\n",
    "inter-variables : le modèle apprend quelles variables sont corrélées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── VanillaTransformer ────────────────────────────────────────────────────\n",
    "class VanillaTransformer(nn.Module):\n",
    "    def __init__(self, window, n_exo, d_model, n_heads, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(1 + n_exo, d_model)\n",
    "        self.pos_enc    = PositionalEncoding(d_model, dropout=dropout)\n",
    "        enc = nn.TransformerEncoderLayer(d_model, n_heads, d_model*4, dropout,\n",
    "                                          batch_first=True, norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, n_layers)\n",
    "        self.head = nn.Sequential(nn.Linear(d_model, d_model//2), nn.GELU(),\n",
    "                                   nn.Dropout(dropout), nn.Linear(d_model//2, 1))\n",
    "\n",
    "    def forward(self, x_endo, x_exo):\n",
    "        x   = torch.cat([x_endo.unsqueeze(-1), x_exo], dim=-1)  # (B, W, 1+n_exo)\n",
    "        tok = self.pos_enc(self.input_proj(x))\n",
    "        return self.head(self.encoder(tok)[:, -1, :]).squeeze(-1)\n",
    "\n",
    "\n",
    "# ── PatchTST ──────────────────────────────────────────────────────────────\n",
    "class PatchTST(nn.Module):\n",
    "    def __init__(self, window, patch_size, d_model, n_heads, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.patch_emb = PatchEmbedding(window, patch_size, d_model)\n",
    "        self.pos_enc   = PositionalEncoding(d_model, dropout=dropout)\n",
    "        enc = nn.TransformerEncoderLayer(d_model, n_heads, d_model*4, dropout,\n",
    "                                          batch_first=True, norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, n_layers)\n",
    "        n_patches = window // patch_size\n",
    "        self.head = nn.Sequential(nn.Flatten(), nn.Linear(n_patches*d_model, d_model),\n",
    "                                   nn.GELU(), nn.Dropout(dropout), nn.Linear(d_model, 1))\n",
    "\n",
    "    def forward(self, x_endo, x_exo=None):\n",
    "        tok = self.pos_enc(self.patch_emb(x_endo))\n",
    "        return self.head(self.encoder(tok)).squeeze(-1)\n",
    "\n",
    "\n",
    "# ── TimeXer ───────────────────────────────────────────────────────────────\n",
    "class TimeXerLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn  = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn  = nn.Sequential(nn.Linear(d_model, d_model*4), nn.GELU(),\n",
    "                                   nn.Dropout(dropout), nn.Linear(d_model*4, d_model))\n",
    "        self.n1, self.n2, self.n3 = nn.LayerNorm(d_model), nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, endo_tok, exo_tok):\n",
    "        h = self.n1(endo_tok)\n",
    "        h, _ = self.self_attn(h, h, h)\n",
    "        endo_tok = endo_tok + self.drop(h)\n",
    "        h = self.n2(endo_tok)\n",
    "        h, _ = self.cross_attn(h, exo_tok, exo_tok)\n",
    "        endo_tok = endo_tok + self.drop(h)\n",
    "        h = self.n3(endo_tok)\n",
    "        return endo_tok + self.drop(self.ffn(h))\n",
    "\n",
    "\n",
    "class TimeXer(nn.Module):\n",
    "    def __init__(self, window, n_exo, patch_size, d_model, n_heads, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.endo_emb = PatchEmbedding(window, patch_size, d_model)\n",
    "        self.exo_emb  = ExoEmbedding(n_exo, d_model)\n",
    "        self.pos_endo = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.pos_exo  = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.layers   = nn.ModuleList([TimeXerLayer(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        n_patches = window // patch_size\n",
    "        self.head = nn.Sequential(nn.Flatten(), nn.Linear(n_patches*d_model, d_model),\n",
    "                                   nn.GELU(), nn.Dropout(dropout), nn.Linear(d_model, 1))\n",
    "\n",
    "    def forward(self, x_endo, x_exo):\n",
    "        endo_tok = self.pos_endo(self.endo_emb(x_endo))\n",
    "        exo_tok  = self.pos_exo(self.exo_emb(x_exo))\n",
    "        for layer in self.layers:\n",
    "            endo_tok = layer(endo_tok, exo_tok)\n",
    "        return self.head(self.norm(endo_tok)).squeeze(-1)\n",
    "\n",
    "\n",
    "# ── iTransformer ──────────────────────────────────────────────────────────\n",
    "class iTransformer(nn.Module):\n",
    "    def __init__(self, window, n_exo, d_model, n_heads, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        n_vars = 1 + n_exo\n",
    "        self.var_emb = nn.Linear(window, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=n_vars+1, dropout=dropout)\n",
    "        enc = nn.TransformerEncoderLayer(d_model, n_heads, d_model*4, dropout,\n",
    "                                          batch_first=True, norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, n_layers)\n",
    "        self.head = nn.Sequential(nn.Linear(d_model, d_model//2), nn.GELU(),\n",
    "                                   nn.Dropout(dropout), nn.Linear(d_model//2, 1))\n",
    "\n",
    "    def forward(self, x_endo, x_exo):\n",
    "        x_mv = torch.cat([x_endo.unsqueeze(-1), x_exo], dim=-1).permute(0, 2, 1)  # (B, n_vars, W)\n",
    "        tok  = self.pos_enc(self.var_emb(x_mv))\n",
    "        tok  = self.encoder(tok)\n",
    "        return self.head(tok[:, 0, :]).squeeze(-1)  # token endogène (index 0)\n",
    "\n",
    "\n",
    "# ── Instanciation et comptage de paramètres ───────────────────────────────\n",
    "MODELS_DEF = {\n",
    "    'VanillaTransformer': VanillaTransformer(WINDOW, N_EXO, D_MODEL, N_HEADS, N_LAYERS, DROPOUT),\n",
    "    'PatchTST'          : PatchTST(WINDOW, PATCH_SIZE, D_MODEL, N_HEADS, N_LAYERS, DROPOUT),\n",
    "    'TimeXer'           : TimeXer(WINDOW, N_EXO, PATCH_SIZE, D_MODEL, N_HEADS, N_LAYERS, DROPOUT),\n",
    "    'iTransformer'      : iTransformer(WINDOW, N_EXO, D_MODEL, N_HEADS, N_LAYERS, DROPOUT),\n",
    "}\n",
    "\n",
    "print(f'{\"Modèle\":<22} | {\"Paramètres\":>12}')\n",
    "print('-' * 36)\n",
    "for name, m in MODELS_DEF.items():\n",
    "    print(f'{name:<22} | {sum(p.numel() for p in m.parameters()):>12,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s7",
   "metadata": {},
   "source": [
    "## 7 — Boucle d'entraînement commune\n",
    "\n",
    "Même pipeline pour tous les modèles :\n",
    "- **AdamW** + weight decay\n",
    "- **CosineAnnealingLR** — décroissance progressive vers LR_min\n",
    "- **Early stopping** sur la loss train\n",
    "- **Gradient clipping** (norme max = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-trainfn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, epochs=EPOCHS, lr=LR, patience=PATIENCE, verbose=True):\n",
    "    model = model.to(DEVICE)\n",
    "    opt   = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs, eta_min=lr/100)\n",
    "    crit  = nn.MSELoss()\n",
    "\n",
    "    best_loss, best_state, no_improve = float('inf'), None, 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for x_endo, x_exo, y in train_dl:\n",
    "            x_endo, x_exo, y = x_endo.to(DEVICE), x_exo.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(x_endo, x_exo), y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item()\n",
    "        sched.step()\n",
    "        avg_loss = epoch_loss / len(train_dl)\n",
    "        history.append(avg_loss)\n",
    "\n",
    "        if avg_loss < best_loss - 1e-6:\n",
    "            best_loss  = avg_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                if verbose: print(f'  Early stopping à l\\'epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if verbose and epoch % 50 == 0:\n",
    "            print(f'  Epoch {epoch:4d}/{epochs}  |  Loss : {avg_loss:.6f}')\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "    return model, history\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def autoregressive_predict(model, all_endo, all_exo, test_start, window, pred_len):\n",
    "    \"\"\"Prédiction autorégressiive sur pred_len jours.\"\"\"\n",
    "    model.eval()\n",
    "    endo_win = list(all_endo[test_start - window : test_start])\n",
    "    preds_norm = []\n",
    "\n",
    "    for step in range(pred_len):\n",
    "        x_endo = torch.tensor(endo_win, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "        exo_start = test_start + step - window\n",
    "        x_exo  = torch.tensor(\n",
    "            all_exo[exo_start : test_start + step], dtype=torch.float32\n",
    "        ).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        pred_n = model(x_endo, x_exo).item()\n",
    "        preds_norm.append(pred_n)\n",
    "        endo_win.pop(0)\n",
    "        endo_win.append(pred_n)\n",
    "\n",
    "    preds = scaler_endo.inverse_transform(np.array(preds_norm).reshape(-1, 1)).flatten()\n",
    "    return preds\n",
    "\n",
    "\n",
    "print('Fonctions d\\'entraînement et d\\'inférence définies ✓')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s8",
   "metadata": {},
   "source": [
    "## 8 — Entraînement et évaluation des 4 modèles\n",
    "\n",
    "Chaque modèle est entraîné avec les **mêmes hyperparamètres** pour une comparaison équitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-trainloop",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_real = df[TARGET_COL].values[-N_TEST:]\n",
    "\n",
    "results   = []\n",
    "histories = {}\n",
    "\n",
    "for name, model in MODELS_DEF.items():\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f' Entraînement : {name}')\n",
    "    print(f'{\"=\"*50}')\n",
    "\n",
    "    trained, history = train_model(model, train_dl, verbose=True)\n",
    "    histories[name]  = history\n",
    "\n",
    "    preds = autoregressive_predict(trained, all_endo, all_exo,\n",
    "                                    test_start=test_start, window=WINDOW, pred_len=N_TEST)\n",
    "\n",
    "    mae  = mean_absolute_error(real_real, preds)\n",
    "    mape = mean_absolute_percentage_error(real_real, preds) * 100\n",
    "    rmse = np.sqrt(np.mean((real_real - preds) ** 2))\n",
    "    r2   = r2_score(real_real, preds)\n",
    "\n",
    "    results.append(dict(model=name, preds=preds, MAE=mae, MAPE=mape, RMSE=rmse, R2=r2))\n",
    "    print(f'  → MAE={mae:.1f}  MAPE={mape:.1f}%  RMSE={rmse:.1f}  R²={r2:.4f}')\n",
    "\n",
    "# ── Résumé comparatif ─────────────────────────────────────────────────────\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f' RÉSUMÉ COMPARATIF — {TEST_LABEL}')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'  {\"Modèle\":<22} | {\"MAE\":>6}  {\"MAPE\":>7}  {\"RMSE\":>6}  {\"R²\":>7}')\n",
    "print(f'  {\"-\"*55}')\n",
    "for r in sorted(results, key=lambda x: x['MAE']):\n",
    "    print(f'  {r[\"model\"]:<22} | {r[\"MAE\"]:>6.1f}  {r[\"MAPE\"]:>6.1f}%  {r[\"RMSE\"]:>6.1f}  {r[\"R2\"]:>7.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s9",
   "metadata": {},
   "source": [
    "## 9 — Métriques individuelles (format standardisé)\n",
    "\n",
    "Format identique aux autres notebooks pour faciliter la comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    print(f'╔{\"═\" * 52}╗')\n",
    "    print(f'║  Métriques {r[\"model\"]:<{40}}║')\n",
    "    print(f'╠{\"═\" * 52}╣')\n",
    "    print(f'║  MAE   (Erreur absolue moyenne)  : {r[\"MAE\"]:>7.1f} réserv.   ║')\n",
    "    print(f'║  MAPE  (Erreur % moyenne)        : {r[\"MAPE\"]:>6.2f}%            ║')\n",
    "    print(f'║  RMSE  (Racine erreur quadrat.)  : {r[\"RMSE\"]:>7.1f} réserv.   ║')\n",
    "    print(f'║  R²    (Coefficient det.)        : {r[\"R2\"]:>7.4f}             ║')\n",
    "    print(f'╚{\"═\" * 52}╝')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s10",
   "metadata": {},
   "source": [
    "## 10 — Visualisations comparatives\n",
    "\n",
    "Trois graphiques standardisés :\n",
    "1. **Prédictions vs Réel** — courbes jour par jour\n",
    "2. **Courbes de convergence** — loss MSE par epoch (échelle log)\n",
    "3. **Barplot MAE & RMSE** — comparaison directe inter-modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f'Transformers — Comparaison des 4 modèles ({TEST_LABEL})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# ── 1 : Prédictions vs Réel ───────────────────────────────────────────────\n",
    "ax = axes[0]\n",
    "ax.plot(range(N_TEST), real_real, 'ko-', lw=2.5, ms=8, label='Réel', zorder=5)\n",
    "for r in results:\n",
    "    ax.plot(range(N_TEST), r['preds'], 'o--', lw=1.8, ms=6,\n",
    "            color=MODEL_COLORS[r['model']], label=r['model'], alpha=0.85)\n",
    "ax.set_xticks(range(N_TEST))\n",
    "ax.set_xticklabels(DAY_LABELS)\n",
    "ax.set_ylabel(TARGET_COL)\n",
    "ax.set_title('Prédictions vs Réel')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# ── 2 : Courbes de convergence ────────────────────────────────────────────\n",
    "ax = axes[1]\n",
    "for name, hist in histories.items():\n",
    "    smooth = pd.Series(hist).rolling(5, min_periods=1).mean()\n",
    "    ax.plot(smooth, lw=1.8, color=MODEL_COLORS[name], label=name)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss (normalisé)')\n",
    "ax.set_title('Courbes de convergence')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_yscale('log')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# ── 3 : Barplot MAE & RMSE ────────────────────────────────────────────────\n",
    "ax = axes[2]\n",
    "model_names = [r['model'].replace('Transformer', 'Tr.') for r in results]\n",
    "maes  = [r['MAE']  for r in results]\n",
    "rmses = [r['RMSE'] for r in results]\n",
    "x = np.arange(len(model_names))\n",
    "w = 0.35\n",
    "b1 = ax.bar(x - w/2, maes,  w, label='MAE',  alpha=0.85,\n",
    "            color=[MODEL_COLORS[r['model']] for r in results])\n",
    "b2 = ax.bar(x + w/2, rmses, w, label='RMSE', alpha=0.55,\n",
    "            color=[MODEL_COLORS[r['model']] for r in results])\n",
    "ax.bar_label(b1, fmt='%.1f', fontsize=8, padding=2)\n",
    "ax.bar_label(b2, fmt='%.1f', fontsize=8, padding=2)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, fontsize=9, rotation=15, ha='right')\n",
    "ax.set_ylabel(f'{TARGET_COL} (erreur)')\n",
    "ax.set_title('MAE & RMSE sur le test')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_PATH, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure sauvegardée → {IMG_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr-s11",
   "metadata": {},
   "source": [
    "## 11 — Tableau détaillé jour par jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Jour\":<10} {\"Réel\":>6}', end='')\n",
    "for r in results:\n",
    "    short = r['model'].replace('Transformer', 'Tr.')\n",
    "    print(f'  {short:>16}', end='')\n",
    "print()\n",
    "print('-' * (16 + 6 + len(results) * 18))\n",
    "\n",
    "for j, day in enumerate(DAY_LABELS):\n",
    "    print(f'{day:<10} {real_real[j]:>6.0f}', end='')\n",
    "    for r in results:\n",
    "        pred = r['preds'][j]\n",
    "        err  = pred - real_real[j]\n",
    "        print(f'  {pred:>8.0f} ({err:>+5.0f})', end='')\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(f'{\"Modèle\":<22} | {\"MAE\":>6}  {\"MAPE\":>7}  {\"RMSE\":>6}  {\"R²\":>7}')\n",
    "print('-' * 52)\n",
    "for r in sorted(results, key=lambda x: x['MAE']):\n",
    "    print(f'{r[\"model\"]:<22} | {r[\"MAE\"]:>6.1f}  {r[\"MAPE\"]:>6.1f}%  {r[\"RMSE\"]:>6.1f}  {r[\"R2\"]:>7.4f}')"
   ]
  }
 ]
}